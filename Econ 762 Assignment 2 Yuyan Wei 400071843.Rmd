---
title: "Econ 762 Assignment 2"
author: "Yuyan Wei 400071843"
date: "Feb 2nd, 2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Question 3

a.Produce a time plot for the series.
```{r}
set.seed(42)
y<-arima.sim(n=100,list(ar=0.6,sd=1))
plot(y)
```

If $\phi_1$ get smaller, the plot will become more intense with more upward and downward fluctuation, in other words, more stationary, vice versa.


b.Write your own code to generate data from an MA(1) model with $\theta_1=0.6$ and $\sigma^2=1$.

c.Produce a time series plot for the simulated series. How does the plot change as you change $\theta_1$?
```{r}
set.seed(42)
e<-ts(rnorm(101))##assume e_t ~iid N(0,1)
z<-e+0.6*lag(e)##generate y=e_t+0.6e_t
plot(z)

```
It is clear that if $\theta_1$ get smaller, the magnitude of the fluatuation get smaller, vice versa.

d.Graph these two time series then compare and contrast them.
```{r}
set.seed(42)
x<-arima.sim(n=100,list(ar=0.6,ma=0.6,sd=1))##generate ARIMA(1,1)
plot(x)
y<-arima.sim(n=100,list(c(2,0,0),ar1=-0.5,ar2=0.3,sd=1))##generate AR(2)
plot(y)
lines(x,col=1,lty=1)
lines(y,col=2,lty=2)
legend("topleft",c("ARMA(1,1)","AR(2)"),col=1:2,lty=1:2,bty="n")
```
From the graph above, it is clear that both of the two time series are stationary.




Question 4

a. Describe and plot the time series. What is the frequency of of the series (i.e.number of samples per unit of time) and the unit of time? What is the start and end date?
```{r}
require(fpp)
data(condmilk)
condmilk
sum(condmilk)
plot(condmilk)
frequency(condmilk)
date=as.numeric(time(condmilk))
date[1]
date[120]

```
The frequency of the series is 12 , the unit of time is month; the start date is January 1971 and the end date is December 1980.  


b. Is the series stationary? If not, find an appropriate differencing which yields a stationary series (differencing with option lag=12 when calling diff() is required).
```{r}
adf.test(condmilk)
```

From the acf test, we can reject the null hypothesis and believe the series is stationary.


c. Identify a few ARIMA(p,d,q)(P,D,Q)m models manually that might be useful for describing the time series (do not use the auto.arima() function here).
Because the series is stationary, we can set d = 0, and use ACF and PACF to infer q and p, respectively. 
```{r}
acf(condmilk,lag.max=100)
pacf(condmilk)
#According the the plots, I infer the following models
arima(condmilk,order=c(1,0,0))
arima(condmilk,order=c(1,0,1))
arima(condmilk,order=c(1,0,0),seasonal=c(1,0,1))
arima(condmilk,order=c(1,0,0),seasonal=c(2,0,0))
```

d. Which of your models is the best according to their AIC values?

The AIC of the third model is the lowest, thus the third model is the best among these four.



e. Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.
```{r}
model3=arima(condmilk,order=c(1,0,0),seasonal=c(1,0,1))
tsdiag(model3)
```
It is clear that the residuals resemble white noise very well. It is reasonable to believe that  the third model fits well.


f. Forecast the next 24 months of data using your preferred model. Comment on the forecasts and their confidence intervals.
```{r}
plot(forecast(model3,h=24))
```
The forecasts gives a reasonable of the pattern of the time series and the confidence interval is not very large compared to the range of data from preceeding years.




Question 5

$$Cov(\epsilon_t,\epsilon_{t-s})=\gamma_s=E[(\epsilon_t-E[\epsilon_t])(\epsilon_{t-s}-E[\epsilon_{t-s}])]=E[\epsilon_t \epsilon_{t-s}]=E[\epsilon_{t-s}(\rho\epsilon_{t-1}+u_t)]$$
$$=\rho E[\epsilon_{t-s}\epsilon_{t-1}]+E[\epsilon_{t-s}u_t]=\rho\gamma_{s-1}=\rho^s\gamma_0$$

as $\gamma_0=Var(\epsilon_t)=var(u_t)/{1-\rho^2}=\sigma_u^2/{1-\rho^2}$

therefore, we have$\rho_s=\frac{\rho^s \sigma_u^2}{1-\rho^2}$



Question 6

a.
$$Var(y_t) = E[(y_t - E(y_t))^2] = E[(y_t)^2] = E[y_t(\beta \times y_{t-1} + u_t)]
=\beta E[y_ty_{t-1}]+E[y_tu_t]
=\beta\gamma_1+\sigma^2$$


$$Cov[y_t,y_{t-1}]=\gamma_1=E[(y_t-E(y_t))(y_{t-1}-E(y_{t-1})]=E[y_ty_{t-1}]=E[y_t(\beta y_{t-1}+u_t]
=\beta E[y_{t-1}^2]+E[y_{t-1}u_t]=\beta\gamma_0=\beta\frac{\sigma_{u}^2}{1-\beta^2}$$

$$\rho_1=\frac{Cov[y_t,y_{t-1}]}{Var(y_t)}=\frac{\beta\gamma_0}{\beta \gamma_1+\sigma^2}$$
$$Cov[y_t,y_{t-2}]=\gamma_2=\beta\gamma_1$$
$$\rho_2=\frac{Cov[y_t,y_{t-2}]}{Var(y_t)}=\frac{\beta\gamma_1}{\beta \gamma_1+\sigma^2}$$

b. 
$y_t=\beta+\epsilon_t=\beta+\rho\epsilon_{t-1}+u_t$, Since $\epsilon_{t}$ is an AR(1) process, $E(\epsilon_{t})=0$
$$E(y_{t})=E(\beta+\rho\epsilon_{t-1}+u_{t})=\beta+E(u_{t})=\beta$$
$$var(y_{t})=E[(y_{t}-E(y_{t}))]^2=E(\epsilon_{2})=var(\epsilon_{t})=\frac{\sigma_{u}^2}{1-\rho^2}$$
$$cov(y_{t},y_{t-1})=E[(y_{t}-E(y_{t})][y_{t-1}-E(y_{t-1})]=E[(\beta+\epsilon_{t}-\beta)(\beta+\epsilon_{t-1}-\beta)]=E(\epsilon_{t}\epsilon_{t-1})=\rho\frac{\sigma_{u}^2}{1-\rho^2}$$
$$cov(y_{t},y_{t-2})=E(\epsilon_{t},\epsilon_{t-2})=\rho^2\frac{\sigma_{u}^2}{1-\rho^2}$$
$$corr(y_{t},y_{t-1})=\frac{cov(y_{t},y_{t-1})}{var(y_{t})}=\rho$$
$$corr(y_{t},y_{t-1})=\frac{cov(y_{t},y_{t-1})}{var(y_{t})}=\rho^2$$

c.
$$Var(y_t)=E[(y_t-E[y_t])^2]=E[y_t^2]=E[(u_t+\theta u_{t-1})^2]=E[u_t^2+2\theta u_t u_{t-1}+\theta^2 u_{t-1}^2]=\sigma^2(1+\theta^2)$$
$$Cov(y_t,y_{t-1})=E[(y_t-E[y_t])(y_{t-1}-E[y_{t-1}])] =E[(u_t+\theta u_{t-1})(u_{t-1}+\theta u_{t-2})]$$
$$=E[u_t u_{t-1}+\theta u_t u_{t-2}+\theta u_{t-1}^2+\theta^2 u_{t-1}u_{t-2}]=\theta E[u_{t-1}^2]=\theta\sigma^2$$
$$\rho_1=\frac{\gamma_1}{\gamma_0}=\frac{\theta\sigma^2}{\sigma^2(1+\theta^2)}=\frac\theta{1+\theta^2}$$
$$Cov(y_t,y_t-2)=E[(y_t-E[y_t])(y_{t-2}-E[y_{t-2}])] =E[(u_t+\theta u_{t-1})(u_{t-2}+\theta u_{t-3})]$$
$$=E[u_t u_{t-2}+\theta u_t u_{t-3}+\theta u_{t-1}u_{t-2}+\theta^2 u_{t-1}u_{t-3}]=0$$
$\rho_2=0$


d.
$$\gamma_0=Var[y_t]=E[y_t^2]=E[u_t^2+0.36u_{t-1}^2+0.04u_{t-2}^2+0.01u_{t-3}^2]=1.41E[u_t^2]=1.41\sigma^2$$

$$\gamma_1=Cov(y_t,y_{t-1})=E[(u_t+0.6u_{t-1}+0.2u_{t-2}+0.1u_{t-3})(u_{t-1}+0.6u_{t-2}+0.2u_{t-3}+0.1u_{t-4})]$$
$$=E[0.6u_{t-1}^2+0.12u_{t-2}^2+0.02u_{t-3}^2]=0.74E[u_t^2]=0.74\sigma^2$$
$$\gamma_2=Cov(y_t,y_{t-2})=E[(u_t+0.6u_{t-1}+0.2u_{t-2}+0.1u_{t-3})(u_{t-2}+0.6u_{t-3}+0.2u_{t-4}+0.1u_{t-5})]$$
$$=E[0.2u_{t-2}^2+0.06u_{t-3}^2]=0.26E[u_t^2]=0.26\sigma^2$$
$$\rho_1=\frac{\gamma_1}{\gamma_0}=\frac{0.74\sigma^2}{1.41\sigma^2}\simeq 0.52$$
$$\rho_2=\frac{\gamma_1}{\gamma_0}=\frac{0.26\sigma^2}{1.41\sigma^2}\simeq 0.18$$


Question 10

a.

i.$E[y_t]=E[\mu+\beta y_{t-1}+u_t]=\mu+\beta E[y_{t-1}]+E[u_t]$, because $E[y_t]=E[y_{t-1}]$, we have $E[y_t]=\frac {\mu}{1-\beta}$.

ii.$$Var(y_t)=E[(y_t-E[y_t])^2]=E[y_t^2]=E[y_t (\mu+\beta y_t+u_t)]=\beta E[y_t y_{t-1}]+E[y_t u_t]=\beta\gamma_1+\sigma^2$$

iii. $$Cov(y_t,y_{t-1})=E[(y_t-E[y_t])(y_{t-1}-E[y_{t-1}])] =E[y_{t-1}(\mu+\beta y_{t-1}+u_t]=\beta E[y_{t-1} y_{t-1}]=\beta\gamma_0$$


b.

i.$$E[y_t]=E[\mu+u_t+0.6u_{t-1}+0.2u_{t-2}]=\mu+E[u_t]+0.6E[u_{t-1}]+0.2E[u_{t-2}]=\mu$$

ii.$$Var(y_t)=E[(y_t-E[y_t])^2]=E[y_t^2]=E[u_t^2+0.6^2 u_{t-1}^2+0.2^2 u_{t-2}^2+...]=1.4E[u_t^2]=1.4\sigma^2$$

iii.$$Cov(y_t,y_{t-1})=E[(y_t-E[y_t])(y_{t-1}-E[y_{t-1}])] =E[(u_t+0.6u_{t-1}+0.2u_{t-2})(u_{t-1}+0.6u_{t-2}+0.2u_{t-3})]$$
$$=E[0.6u_{t-1}^2+0.12u_{t-2}^2+...]=0.72\sigma^2$$




Question 13

a. What is the numerical value of the correlation between ??t and ??t???3
when $k =3$, we konw $p =1$, so $k>p$; therefore, $Corr(\epsilon_3,\epsilon_{t-3})=\rho_3=Cor(\epsilon_t,\epsilon_{t-3})=0$.


b. What is the numerical value of Var(??t)
$$Var(y_t)=E[(y_t-E[y_t])^2]=E[y_t^2]=E[\epsilon_t (\rho\epsilon_{t-1}+u+t)]=\rho E[\epsilon_t \epsilon_{t-1}]+E[u_t^2]=0.6\gamma_1+5$$
We also have $\gamma_1=0.6\gamma_0$, combine the two equations, $Var(y_t)=\gamma_0=5/0.64=7.8125$


c. Suppose that E(ut)=10, instead of the usual zero-mean assumption. What is the numerical value of E(??t)?
$E[\epsilon_t]=E[\rho\epsilon_{t-1}+u_t]=0.6E[\epsilon_{t-1}]+E[u_t]$, as$E[\epsilon_t]=E[\epsilon_{t-1}]$ and $E(u_t]=10$, we have $E[\epsilon_t]=10/0.4=25$.





Question 14

a. 
$$Var(y_t)=E[y_t^2]=E[(u_t+0.7u_{t-1}+0.1u_{t-2})^2]=E[u_t^2+0.49u_{t-1}^2+0.01u_{t-2}^2+...]=1.5E[u_t^2]=30$$


b. $$Cor(y_t,y_{t-1})=\frac {Cov(y_t,y_{t-1})} {Var(y_t)} =15.4/30 \simeq 0.51$$


c. $$Cov(y_t,y_{t-1})=E[(y_t-E[y_t])(y_{t-1}-E[y_{t-1}])]=E[[(u_t+0.7u_{t-1}+0.1u_{t-2})(u_{t-1}+0.7u_{t-2}+0.1u_{t-3})]$$
$$=E[0.7u_{t-1}^2+0.07u_{t-2}^2+...]=0.77E[u_t^2]=15.4$$